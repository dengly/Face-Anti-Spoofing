# -*- coding: utf-8 -*-
"""CVProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xB30uWaGJ54wcxUvgoTj5HCOJcFNnfnt
"""

import numpy as np
import cv2
import os
from matplotlib import pyplot as plt
from sklearn.feature_extraction import image
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout,Conv2DTranspose
from keras.layers.normalization import BatchNormalization
import tensorflow as tf
from keras.layers import GlobalAveragePooling2D
from keras.utils import np_utils
from keras.applications import VGG16
from keras.models import Model
from keras.layers.advanced_activations import LeakyReLU
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import cv2








from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

face_cascade = cv2.CascadeClassifier("E:\\Softwares\\Anaconda\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml")
eye_cascade = cv2.CascadeClassifier('E:\\Softwares\\Anaconda\\Lib\\site-packages\\cv2\\data\\haarcascade_eye.xml')


#creating patch based cnn
model = Sequential()
model.add(Conv2D(50, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(96,96,3),padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Conv2D(100, (3, 3),strides=(1,1), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))
model.add(Conv2D(150, (3, 3),strides=(1,1), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))
model.add(Conv2D(200, (3, 3),strides=(1,1), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))
model.add(Conv2D(250, (3, 3),strides=(1,1), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))
model.add(Flatten())
model.add(Dense(1000, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(400, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(2,activation='softmax'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

import h5py

hf = h5py.File('train_data (2).h5', 'r')
print(hf.keys())
n1 = hf.get('dataset_1')
n1 = np.array(n1)
print(np.shape(n1))
n2=hf.get('dataset_2')
n2=np.array(n2)
model.fit(n1, n2,
          batch_size=32,
          epochs=5,
          verbose=1)

from keras.models import model_from_json
hf1 = h5py.File('test_data (1).h5', 'r')
print(hf1.keys())
n11 = hf1.get('test_dataset_1')
n11 = np.array(n11)
print(np.shape(n11))
n21=hf1.get('test_dataset_2')
n21=np.array(n21)
score=model.evaluate(n11,n21,verbose=0)
print(score)

model_json = model.to_json()
with open("model1.json", "w") as json_file:
    json_file.write(model_json)
model.save_weights("model.h5")
from google.colab import files
files.download('model.json')
.
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
uploaded = drive.CreateFile({'title': 'm_odel.json'})
uploaded.SetContentFile('model.json')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

model_json = model.to_json()
with open("model1.json", "w") as json_file:
    json_file.write(model_json)

model.save_weights("model.h5")
uploaded = drive.CreateFile({'title': 'model.json'})
uploaded.SetContentFile('model1.json')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))
uploaded = drive.CreateFile({'title': 'model_weights.h5'})
uploaded.SetContentFile('model.h5')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))
